{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# CS187\n", "\n", "## Lab0-1: Tensors and vectorization\n", "\n", "Many of the data-heavy approaches to NLP are enabled by advances in parallel processing that make what were once intractable computations practical. This notebook demonstrates the issue and the `torch` technologies that apply to it, especially the \"tensor\" data type."]}, {"cell_type": "code", "execution_count": null, "id": "d82358aa", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2020/lab0-1.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import random\n", "\n", "from timeit import timeit"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Numeric vectors can be implemented in Python in many ways. Most directly, Python provides a built-in `list` data type, which we could use to implement a vector. Here, we generate a couple of example vectors as lists each containing 1000 integers between 0 and 99."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a1 = random.choices(range(100), k=1000)\n", "a2 = random.choices(range(100), k=1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### An example: dot product\n", "\n", "The dot product of two vectors is the sum of their componentwise product, which can be calculated with a simple for-loop."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def dotproduct(v1, v2):\n", "    sum = 0\n", "    for i in range(len(v1)):\n", "         sum += v1[i] * v2[i]\n", "    return sum"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dotproduct_result = dotproduct(a1, a2)\n", "dotproduct_result"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can test the efficiency of this approach to implementing vectors by computing a large dot product many times. (We use the `timeit` function that we imported from the `timeit` library to return the time in seconds to perform 100,000 repetitions of the dot product computation.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["example_time = timeit('dotproduct(a1, a2)', number=100000, globals=globals())\n", "f\"It took {example_time} seconds.\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["As it turns out, performing this vector computation is quite slow -- it probably took several seconds -- because the for loop over the list data structure  computes sequentially. Instead, we can use a data type engineered especially for such vector and array computations to improve performance. Such data types include Python arrays, `numpy` arrays, and [`torch` tensors](https://pytorch.org/docs/stable/tensors.html). The latter are especially designed for the kinds of computations found in machine learning algorithms, so we will use them throughout the course. You can read (a lot) more about tensors in [the official documentation](https://pytorch.org/docs/stable/tensors.html).\n", "\n", "We construct a couple of one-dimensional tensors for the examples above."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t1 = torch.tensor(a1)\n", "t2 = torch.tensor(a2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tensor properties\n", "\n", "Tensors have three properties that are especially useful (but potentially confusing when you first start working with them):\n", "\n", "+ Componentwise operation: Many operations on tensors work component by component instead of all at once.\n", "+ Broadcast: Operations can broadcast individual elements to each component of a tensor.\n", "+ Reshaping: Tensors can be reshaped to present the same elememtns in different configurations.\n", "+ Special operations: Tensors have mehtods implementing certain operations especially efficiently.\n", "\n", "We give examples of each:\n", "\n", "#### Componentwise operation\n", "\n", "When we add two tensors of the same shape with the `+` operator, the summation percolates down to the individual comonents. For example,"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a3 = [1, 2, 3]\n", "a4 = [4, 5, 6]\n", "t3 = torch.tensor(a3)\n", "t4 = torch.tensor(a4)\n", "\n", "t3 + t4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is quite different from, say, lists, which perform a completely different operation -- concatenation -- when summed with the `+` operator."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a3 + a4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Broadcast\n", "\n", "Related, adding a scalar to a tensor \"broadcasts\" the scalar addition operation to each element."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(t3 + 5)\n", "print(5 + t3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again, compare with how lists work."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["raises-exception"]}, "outputs": [], "source": ["a3 + 5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Reshaping\n", "\n", "Finally, tensors can be reshaped so that their elements appear in a different configuration. The `view` method is often used to carry out the reshaping. For instance, we start with the following 3 by 4 tensor."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t5 = torch.tensor([[11, 12, 13, 14],\n", "                   [21, 22, 23, 24],\n", "                   [31, 32, 33, 34]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can view the elements as a 4 by 3 tensor, or a 2 by 2 by 3 tensor, or a 3 by 1 by 4 tensor."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(t5.view(4, 3))\n", "print(t5.view(2, 2, 3))\n", "print(t5.view(3, 1, 4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Special operations\n", "\n", "Tensors have a large set of methods defined on them that work especially efficiently, for instance, taking the sum of the elements, or the minimum."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t5.sum()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t5.min()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also take the min/max with respect to a particular dimension. For example, to find the minimum for each row, we can take the min with respect to the second dimension (note that we need to pass 1 since dimension is also 0-indexed). Note that we need to use `.values` since this function would also return the indices where the minimums are (you can check this by using `.indicies` instead of `.values`.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t5.min(1).values"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!--\n", "BEGIN QUESTION\n", "name: max_val\n", "-->\n", "Can you find the maximum value of each column?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TODO -- Implement a function to return the max value of each column.\n", "def max_col(v):\n", "..."]}, {"cell_type": "code", "execution_count": null, "id": "2e0225a1", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"max_val\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["### Vectorized dot product\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: dotprod\n", "-->\n", "Using these tensor techniques, and noting the examples above, reimplement a version of `dotproduct` that has no `for` loops. **Hint:** Your code should be *very short*."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TODO -- Implement a vectorized dot product, which should be much faster.\n", "def dotproduct_v(v1, v2):\n", "..."]}, {"cell_type": "code", "execution_count": null, "id": "967c3df8", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"dotprod\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This vectorized version should be *much* faster, perhaps a couple of orders of magnitude."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["timeit('dotproduct_v(t1, t2)', number=10000, globals=globals())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Vectorized computations over multidimensional tensors\n", "\n", "Tensors aren't limited to one dimension, and this same vectorization trick applies to multidimensional tensors. In fact, because of vectorization, we can specify computations over multidimensional tensors that look like the kinds of things you've seen in linear algebra. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### An example: all-paths shortest path\n", "As a concrete example to give you some practice, consider the algorithm for computing shortest paths in a graph of $n$ nodes. We'll represent the graph as an $n \\times n$ matrix $A$ where $A_{ij}$ is the distance from node $i$ to node $j$. (Thus, this is a directed graph, and the distances needn't be symmetric.) Here's an example:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from math import inf\n", "distances = torch.tensor(\n", "              [[0, 1,   2, 6],\n", "               [1, 0,   2, inf],\n", "               [2, 2,   0, 3],\n", "               [5, inf, 3, 0]])"]}, {"cell_type": "raw", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["(We use `inf` for an infinite distance, that is, for nodes that are not connected with an edge.) \n", "\n", "In this graph, the distance from node 0 to node 3 is 6, but by going through node 2, we can shorten the path to 5. In general, consider [the *minplus* operation](https://en.wikipedia.org/wiki/Min-plus_matrix_multiplication) ($\\star$) on two square matrices $A$ and $B$:\n", "$$(A \\star B)_{ij} = \\min_k A_{ik} + B_{kj}$$\n", "If $A$ and $B$ are two graphs over the same nodes (but with different distances), then $A \\star B$ is the graph that shows the best way to get from one node to another by traversing an edge from the first graph $B$ and then an edge from the second graph $B$.\n", "\n", "Here's an implementation of this operation `minplus` using `for` loops."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def minplus_loop(A, B):\n", "    R = torch.zeros_like(A)\n", "    (arows, acols), (brows, bcols) = A.size(), B.size()\n", "    assert arows == acols and brows == bcols and arows == brows\n", "    for i in range(arows):\n", "        for j in range(arows):\n", "            min = inf\n", "            for k in range(arows):\n", "                if A[i,k] + B[k,j] < min:\n", "                    min = A[i,k] + B[k,j]\n", "                R[i,j] = min\n", "    return R"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using this, we can compute some better ways of getting among the nodes in the`distances` graph. For paths of length at most 2, we can compute"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["minplus_loop(distances, distances)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that in this graph, the distance from node 0 to node 3 is now only 5, and there are paths between nodes 1 and 3.\n", "\n", "We can compute the minimum distance between any two nodes by repeating this minplus process until no further distance reductions are possible and the graph has reached a stable point (the so-called \"fixpoint\"). We return the fixpoint graph and the noumber of rounds of minplus that were needed to reach it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def minplus_fp(X):\n", "    rounds = 0\n", "    lastY = torch.zeros_like(X)\n", "    Y = X\n", "    while not(torch.equal(Y, lastY)):\n", "        lastY = Y\n", "        Y = minplus_loop(Y, Y)\n", "        rounds += 1\n", "    return Y, rounds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["minplus_fp(distances)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It turns out that after just the two rounds, the fixpoint is reached.\n", "\n", "> **Digression:** The complexity of `minplus` as implemented is $O(n^3)$, and the fixpoint computation may need up to $\\log n$ calls to `minplus` to converge, so the overall complexity is $O(n^3 \\log n)$. More efficient algorithms are known, especially the Floyd-Warshall algorithm for the all-paths versions and Dijkstra's algorithm for the single-source version. But efficiency is not our main aim here.\n", "\n", "Let's try a bigger example, a graph with 10 nodes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def random_square_tensor(size):\n", "    X = torch.rand(size, size)\n", "    for i in range(size):\n", "        X[i, i] = 0 \n", "    return X\n", "\n", "X = random_square_tensor(10)\n", "X"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["minplus_fp(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's talk about the \"loop\"-y implementation of the `minplus` function. If we're a little cleverer, we can use list comprehensions to hide the computation of the minimum, but we're still doing the whole computation sequentially."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def minplus_loop2(A, B):\n", "    R = torch.zeros_like(A)\n", "    (arows, acols), (brows, bcols) = A.size(), B.size()\n", "    assert arows == acols and brows == bcols and arows == brows\n", "    for i in range(arows):\n", "        for j in range(arows):\n", "            R[i,j] = min([A[i,k] + B[k,j] for k in range(arows)])\n", "    return R"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `torch`-y way to perform this computation is to rely on vectorized computations. Doing so is a bit tricky however. We need to reshape the matrices a bit. We start by turning the two-dimensional $A$ matrix from a $r \\times c$ matrix (rows by columns, which may differ in the general case) into a three-dimensional $r \\times 1 \\times c$ matrix. Here's the result of that operation on the $4 \\times 4$ `distances` matrix, using [the torch `view` method](https://pytorch.org/docs/stable/tensor_view.html)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["distances.view(4, 1, 4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now each row in the matrix contains a single element, a vector that corresponds to the column in the original. We'll do a similar operation on $B$ (again, the `distances` matrix in our example, but this time reshaping the matrix to be $1 \\times c \\times r$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["distances.view(1, 4, 4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now if we add these two matrices componentwise, what do we get? Each of the four first-index elements in $A$ corresponds to only a single first-index element in $B$, so they'll be added componentwise; that single element will be \"broadcast\" to each of the rows. Thus, the initial element in $A$ (`[[0., 1., 2., 6.]]` in the example) is to be added to the single element in $B$ (`[[0., 1., 2., 6.], [1., 0., 2., inf], [2., 2., 0., 3.], [5., inf, 3., 0.]]` in the example). Proceeding, the same thing happens again, the single element in that $A$ element `[0., 1., 2., 6.]` is broadcast to each of the four elements in $B$ (the first of which is also `[0., 1., 2., 6.]`). Now these are the same size, so they are added elementwise, yielding `[0., 2., 4., 12.]`. Similarly for each of the other three elements in the reshaped $B$ matrix element. When all is said and done, we'd have an $r \\times r \\times c$ matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["summed = distances.view(4, 1, 4) + distances.view(1, 4, 4)\n", "summed"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now we just need to take the minimum of each of the $r \\times r$ elements (using the `min` method) to get the final result of the `minplus` operation.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: minplus_v_example\n", "-->\n", "Calculate the result of the `minplus` by performing appropriate operations on `summed` to yield a tensor that should be identical to the `minplus_loop(distances, distances)` example above."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "result = ...\n", "result"]}, {"cell_type": "code", "execution_count": null, "id": "4de6161b", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"minplus_v_example\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now that you've seen an example of how the matrices can be reshaped and operated on to implement the `minplus` operation, write a function `minplus_v` (a \"vectorized\" version of `minplus_loop`) that computes the minplus of two matrices without any looping constructs.\n", "<!--\n", "BEGIN QUESTION\n", "name: minplus_v\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def minplus_v(A, B):\n", "    ## BEGIN SOLUTION NO PROMPT\n", "    (arows, acols), (brows, bcols) = A.size(), B.size()\n", "    assert brows == acols and brows == bcols and arows == brows\n", "    ## END SOLUTION NO PROMPT\n", "    ..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we'll use your vectorized `minplus_v` to implement a vectorized fixpoint calculation, and test the relative speeds."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def minplus_vfp(X):\n", "    rounds = 0\n", "    lastY = torch.zeros_like(X)\n", "    Y = X\n", "    while not(torch.equal(Y, lastY)):\n", "        lastY = Y\n", "        Y = minplus_v(Y, Y)\n", "        rounds += 1\n", "    return Y, rounds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["minplus_vfp(distances)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["example = random_square_tensor(20)\n", "\n", "timeit('minplus_fp(example)', number=10, globals=globals())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["timeit('minplus_vfp(example)', number=10, globals=globals())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you've implemented `minplus_v` correctly, the efficiency difference should be striking. This kind of engineered improvement is the difference between a computation taking a day and one taking a minute."]}, {"cell_type": "markdown", "id": "51fedce2", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "id": "d456b3a6", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}, {"cell_type": "markdown", "id": "620d66df", "metadata": {"deletable": false, "editable": false}, "source": ["## Submission\n", "\n", "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"]}, {"cell_type": "code", "execution_count": null, "id": "6231afc5", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Save your notebook first, then run this cell to export your submission.\n", "grader.export(\"lab0-1.ipynb\")"]}, {"cell_type": "markdown", "id": "e394411f", "metadata": {}, "source": [" "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "title": "CS187 Lab 0-0: Tensors and vectorization"}, "nbformat": 4, "nbformat_minor": 4}